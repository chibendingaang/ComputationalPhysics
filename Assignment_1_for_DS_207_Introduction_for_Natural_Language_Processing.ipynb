{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chibendingaang/ComputationalPhysics/blob/main/Assignment_1_for_DS_207_Introduction_for_Natural_Language_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OerOiObFAes2"
      },
      "source": [
        "# Assignment 1\n",
        "\n",
        "**DS: 207 Introduction to Natural Language Processing**\n",
        "\n",
        "**Due (Feb 6, 16:59 PM)**\n",
        "\n",
        "Development & Design: Debarpan Bhattacharya & Nicy Scaria.\n",
        "\n",
        "Testing: Kinshuk Vasisht"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of this assignment is introduce the basics of text processing, by building a few text classifiers, and learning to represent words.\n",
        "\n",
        "You'll have to add your code wherever you see the comment `# ADD YOUR CODE HERE`. Please make a copy of this assignment, and you can use Google Colab notebooks to work on this. Later, you can download this notebook as a python file and submit it as per the following instructions."
      ],
      "metadata": {
        "id": "a8KnPBvJEzX9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submission Instructions:\n",
        "\n",
        "1. In the notebook, a few example codes are provided, whereas a few are kept blank for you to fill them up.\n",
        "2. The evaluation will be based on the results obtained by functions impleted by you. Do not change the print statements having `EVALUATION` as they are used for auto-evaluation.\n",
        "3. **Submission file(s)**:\n",
        "  \n",
        "  * Save this `.ipynb` to your drive, complete the required code blocks and run the notebook.\n",
        "  * After completing this assignment, download the notebook as`.py`. Name it as `SAPname_SRno_assignment1.py`, where `SAPname` refers to your name as per SAP record, and `SRno` refers to the last 5 digits of your IISc SR number. For example, IISc student with SAP name Twyla Linda (SR no - 04-03-00-10-22-20-1-15329) would name it as `Twyla_Linda_15329_assignment1.py`.\n",
        "  *   The files associated with the Word2Vec training, i.e., the `model.pt`, `vocab.pt`, `word_embeddings.npy` and `loss.json`, will be downloaded and saved in a folder with the name `SAPname_SRno`. Zip this folder along with the `.py` file, save it as `SAPname_SRno_assigment1` and upload on MS Teams. The zip folder should contain: (1) `SAPname_SRno_assignment1.py`, (2) a subfolder called `SAPname_SRno` which will contain `model.pt`, `vocab.pt` , `word_embeddings.npy` and `loss.json`.\n",
        "\n",
        "\n",
        "Because submissions are auto-graded, please ensure that the naming of the files is consistent with the instructions."
      ],
      "metadata": {
        "id": "4qpIrZWGbSK0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAVFm89QjHP6"
      },
      "source": [
        "## Part I Text Classification (TA: Debarpan Bhattacharya)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQQTcvWL7Qtm"
      },
      "source": [
        "**Dataset**:\n",
        "\n",
        "We will dive into a basic text-based sentiment classification task. The dataset consists of sentences with two different kinds of sentiments- `positive`, and `negative` sentiments. Following are a set of examples,\n",
        "\n",
        "* **`positive`**: *I really like your new haircut!*\n",
        "* **`negative`**: *Your new haircut is awful!*\n",
        "\n",
        "The Dataset has a training set (`train_data.csv`- provided), a validation set (`val_data.csv`- provided) and a blind test set (`test_data.csv`- not provided). The notebook uses a `test_data.csv` file, but it is just a duplicate of `val_data.csv`, and the blind `test_data.csv` will replace it while grading your solutions.\n",
        "\n",
        "**Important**: Fix seed as 42 whenever performing any randomized operations, e.g., initializing ML models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download the dataset required for the assignment"
      ],
      "metadata": {
        "id": "b_gUoUztJVwX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download train data\n",
        "!wget -O train_data.csv \"https://docs.google.com/spreadsheets/d/176-KrOP8nhLpoW91UnrOY9oq_-I0XYNKS1zmqIErFsA/gviz/tq?tqx=out:csv&sheet=train_data.csv\"\n",
        "\n",
        "# download validation data\n",
        "!wget -O val_data.csv \"https://docs.google.com/spreadsheets/d/1YxjoAbatow3F5lbPEODToa8-YWvJoTY0aABS9zaXk-c/gviz/tq?tqx=out:csv&sheet=val_data.csv\"\n",
        "\n",
        "# download test data\n",
        "!wget -O test_data.csv \"https://docs.google.com/spreadsheets/d/1YxjoAbatow3F5lbPEODToa8-YWvJoTY0aABS9zaXk-c/gviz/tq?tqx=out:csv&sheet=val_data.csv\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waznd1GRGwFx",
        "outputId": "a9ff8c00-32a3-4e14-c39e-25bf5e455f8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-22 12:47:28--  https://docs.google.com/spreadsheets/d/176-KrOP8nhLpoW91UnrOY9oq_-I0XYNKS1zmqIErFsA/gviz/tq?tqx=out:csv&sheet=train_data.csv\n",
            "Resolving docs.google.com (docs.google.com)... 108.177.11.102, 108.177.11.139, 108.177.11.100, ...\n",
            "Connecting to docs.google.com (docs.google.com)|108.177.11.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/csv]\n",
            "Saving to: ‘train_data.csv’\n",
            "\n",
            "train_data.csv          [            <=>     ]  15.12M  6.62MB/s    in 2.3s    \n",
            "\n",
            "2024-01-22 12:47:32 (6.62 MB/s) - ‘train_data.csv’ saved [15851033]\n",
            "\n",
            "--2024-01-22 12:47:32--  https://docs.google.com/spreadsheets/d/1YxjoAbatow3F5lbPEODToa8-YWvJoTY0aABS9zaXk-c/gviz/tq?tqx=out:csv&sheet=val_data.csv\n",
            "Resolving docs.google.com (docs.google.com)... 108.177.11.102, 108.177.11.139, 108.177.11.100, ...\n",
            "Connecting to docs.google.com (docs.google.com)|108.177.11.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/csv]\n",
            "Saving to: ‘val_data.csv’\n",
            "\n",
            "val_data.csv            [    <=>             ]   5.14M  6.59MB/s    in 0.8s    \n",
            "\n",
            "2024-01-22 12:47:33 (6.59 MB/s) - ‘val_data.csv’ saved [5387545]\n",
            "\n",
            "--2024-01-22 12:47:33--  https://docs.google.com/spreadsheets/d/1YxjoAbatow3F5lbPEODToa8-YWvJoTY0aABS9zaXk-c/gviz/tq?tqx=out:csv&sheet=val_data.csv\n",
            "Resolving docs.google.com (docs.google.com)... 108.177.11.102, 108.177.11.139, 108.177.11.100, ...\n",
            "Connecting to docs.google.com (docs.google.com)|108.177.11.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/csv]\n",
            "Saving to: ‘test_data.csv’\n",
            "\n",
            "test_data.csv           [    <=>             ]   5.14M  6.75MB/s    in 0.8s    \n",
            "\n",
            "2024-01-22 12:47:34 (6.75 MB/s) - ‘test_data.csv’ saved [5387545]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "d7B1k-8XOEqo",
        "outputId": "180224b9-7cb4-401a-d674-47e12a7aff02"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              review sentiment\n",
              "0  I've watched this documentary twice - and alth...  positive\n",
              "1  This is probably the worst movie I've seen in ...  negative\n",
              "2  Superb story of a dedicated young teacher who ...  positive\n",
              "3  <br /><br />Spoilers<br /><br />I'm going to b...  negative\n",
              "4  What can be said, really... \"The Tenant\" is a ...  positive"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7807733d-dcde-4888-8516-cac63c9a9ba8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I've watched this documentary twice - and alth...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>This is probably the worst movie I've seen in ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Superb story of a dedicated young teacher who ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>&lt;br /&gt;&lt;br /&gt;Spoilers&lt;br /&gt;&lt;br /&gt;I'm going to b...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What can be said, really... \"The Tenant\" is a ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7807733d-dcde-4888-8516-cac63c9a9ba8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7807733d-dcde-4888-8516-cac63c9a9ba8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7807733d-dcde-4888-8516-cac63c9a9ba8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a8800913-1d9c-4c9f-b9bc-f2f4ef82e843\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a8800913-1d9c-4c9f-b9bc-f2f4ef82e843')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a8800913-1d9c-4c9f-b9bc-f2f4ef82e843 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "#@title Read data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('train_data.csv')\n",
        "df_val = pd.read_csv('val_data.csv')\n",
        "\n",
        "# Note that we will change the test file\n",
        "# when we grade assignments  ...\n",
        "# For now it is the same as the validation set\n",
        "df_test = pd.read_csv('test_data.csv')\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxi5DtdKOt7e"
      },
      "outputs": [],
      "source": [
        "#@title Prepare training, validation and test data.\n",
        "\n",
        "X_train, y_train = df.review.values.tolist(), df.sentiment.values.tolist()\n",
        "X_val, y_val = df_val.review.values.tolist(), df_val.sentiment.values.tolist()\n",
        "X_test, y_test = df_test.review.values.tolist(), df_test.sentiment.values.tolist()\n",
        "\n",
        "labels = ['negative', 'positive']\n",
        "\n",
        "# converting the sentiment labels into labels\n",
        "# class 0 for negagtive, and class 1 for positive\n",
        "y_train = [labels.index(i) for i in y_train]\n",
        "y_val = [labels.index(i) for i in y_val]\n",
        "y_test = [labels.index(i) for i in y_test]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGiZtZSpQmk5"
      },
      "source": [
        "#### Approach 1: Rule based classification\n",
        "\n",
        "The rule-based classification works using a few hand-crafted rules. In sentiment classification, few words are associated with positive sentiment and few others with negative sentiment. Let's attempt to build a classifier that predicts the sentiment of the reviews based on such words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRt7jQNVaPO2"
      },
      "source": [
        "Let's first write a few rules to extract important features about the input movie review. As an example, we provide a function that counts the number of `good` and `bad` words in the input.\n",
        "\n",
        "Below, you can see a sample rule-based classification system. Later, you will be building one yourself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JT_Et4xpPThR"
      },
      "outputs": [],
      "source": [
        "def sample_extract_features(X):\n",
        "    \"\"\"\n",
        "    Extracts features from a text input.\n",
        "\n",
        "    Args:\n",
        "        X (string): Text input.\n",
        "\n",
        "    Returns:\n",
        "        dictionary: features extracted from X.\n",
        "    \"\"\"\n",
        "\n",
        "    features = {}\n",
        "    X_split = X.split(' ')\n",
        "\n",
        "    # Count the number of \"good words\" and \"bad words\" in the text\n",
        "    good_words = ['love', 'good', 'brilliant', 'fantastic', 'amazing', 'great']\n",
        "    bad_words = ['hate', 'bad', 'horrible', 'awful', 'terrible', 'mess', 'frustating', 'frustatingly']\n",
        "\n",
        "    features['good_word_count'], features['bad_word_count'] = 0, 0\n",
        "    for x in X_split:\n",
        "        if x in good_words:\n",
        "            features['good_word_count'] = features.get('good_word_count', 0) + 1\n",
        "        if x in bad_words:\n",
        "            features['bad_word_count'] = features.get('bad_word_count', 0) + 1\n",
        "\n",
        "    # The \"bias\" value can be set to one, to allow us to assign a \"default\" score to the text\n",
        "    features['bias'] = 1\n",
        "    return features"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_predict(X, feature_weights):\n",
        "    \"\"\"\n",
        "    Classifies the sentiment of a text input.\n",
        "\n",
        "    Args:\n",
        "        X (string): Text input.\n",
        "        feature_weights: weightage of different features.\n",
        "\n",
        "    Returns:\n",
        "        int: binary sentiment represented by 0/1.\n",
        "    \"\"\"\n",
        "\n",
        "    score = 0\n",
        "\n",
        "    # Here we just multiply the feature value\n",
        "    # with its corresponding weight and aggregate\n",
        "    for feat_name, feat_value in sample_extract_features(X).items():\n",
        "        score = score + feat_value * feature_weights[feat_name]\n",
        "\n",
        "    # the prediction is based on whether the aggregated score is above 0 or not\n",
        "    if score > 0:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0"
      ],
      "metadata": {
        "id": "PQea1rsPCCVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sample_features_weights():\n",
        "  \"\"\"\n",
        "    To obtain feature weightage for different features.\n",
        "\n",
        "    Args:\n",
        "        None here.\n",
        "\n",
        "    Returns:\n",
        "        dictionary: feature names and their weightage.\n",
        "    \"\"\"\n",
        "  # Based on the selected features, you can manually assign them weights\n",
        "  feature_weights = {'good_word_count': 1.0, 'bad_word_count': -1.0, 'bias': 0.5}\n",
        "  return feature_weights"
      ],
      "metadata": {
        "id": "Mld2X8uP_1DK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Computing the accuracy of the system\n",
        "\n",
        "def calculate_accuracy(Y_true, Y_pred):\n",
        "    \"\"\"\n",
        "    Calculates accuracy of predictions given the ground truth.\n",
        "\n",
        "    Args:\n",
        "        Y_true (list): Ground truth labels.\n",
        "        Y_pred (list): Predictions.\n",
        "\n",
        "    Returns:\n",
        "        float: Prediction accuracy in range [0.0-100.0].\n",
        "    \"\"\"\n",
        "    correct = 0.0\n",
        "    total = len(Y_true)\n",
        "\n",
        "    # verify if we have the same number of predictions as labels\n",
        "    assert len(Y_true) == len(Y_pred)\n",
        "\n",
        "    # count the number of correct predictions\n",
        "    for y_true, y_pred in zip(Y_true, Y_pred):\n",
        "        if y_true == y_pred:\n",
        "            correct += 1.0\n",
        "\n",
        "    if total > 0:\n",
        "        return 100. * correct / total\n",
        "\n",
        "    # return 0 if there the total number of examples are zero\n",
        "    return 0.0"
      ],
      "metadata": {
        "id": "5soNJvv0AZeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Putting the sample rule-based classifier together\n",
        "\n",
        "# get the sample weights\n",
        "sample_feature_weights = get_sample_features_weights()\n",
        "\n",
        "predictions = []\n",
        "\n",
        "# for each test example, make a prediction\n",
        "for input_example in X_test:\n",
        "    y = sample_predict(input_example, sample_feature_weights)\n",
        "    predictions.append(y)\n",
        "\n",
        "# compute and print the accuracy\n",
        "print (calculate_accuracy(y_test, predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUJxIXK6COeY",
        "outputId": "b174f919-e611-44ed-ba85-bbffbdc737b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can observe here, the model achieves about 60% accuracy (note that the performance of a random classifier would be close to 50%)."
      ],
      "metadata": {
        "id": "p5zIgT9sGejB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build your own rule-based classifier (10 marks)\n",
        "\n",
        "Your have to write your own `extract_features`, `get_feature_weights` and `predict` functions for your rule-based classifier."
      ],
      "metadata": {
        "id": "jUNaPV2KGyB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(X):\n",
        "    \"\"\"\n",
        "    Extracts features from a text input.\n",
        "\n",
        "    Args:\n",
        "        X (string): Text input.\n",
        "\n",
        "    Returns:\n",
        "        dictionary: features extracted from X.\n",
        "    \"\"\"\n",
        "    features = {}\n",
        "    features['bias'] = 1\n",
        "\n",
        "    # ADD YOUR CODE HERE\n",
        "\n",
        "    # please extract features that you think would be useful.\n",
        "\n",
        "    return features"
      ],
      "metadata": {
        "id": "Aio_eGugGuuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SY0ioIklQp-J"
      },
      "outputs": [],
      "source": [
        "def get_feature_weights():\n",
        "    \"\"\"\n",
        "    To obtain feature weightage for different features.\n",
        "\n",
        "    Args:\n",
        "        None here.\n",
        "\n",
        "    Returns:\n",
        "        dictionary: feature names and their weightage.\n",
        "    \"\"\"\n",
        "\n",
        "    feature_weights = {}\n",
        "    feature_weights['bias'] = 0.5\n",
        "\n",
        "    # ADD YOUR CODE HERE\n",
        "\n",
        "    # please manually assign feature weights to the features you've extracted\n",
        "\n",
        "    return feature_weights"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X, feature_weights):\n",
        "    \"\"\"\n",
        "    Classifies the sentiment of a text input.\n",
        "\n",
        "    Args:\n",
        "        X (string): Text input.\n",
        "        feature_weights: weightage of different features.\n",
        "\n",
        "    Returns:\n",
        "        int: binary sentiment represented by 0/1.\n",
        "    \"\"\"\n",
        "\n",
        "    # ADD YOUR CODE HERE\n",
        "\n",
        "    return 1.0"
      ],
      "metadata": {
        "id": "hKqwGoZ-H7x4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Evaluating your rule-based classifier\n",
        "\n",
        "## Please do not change anything in this code block.\n",
        "\n",
        "feature_weights = get_feature_weights()\n",
        "\n",
        "predictions = []\n",
        "\n",
        "for input_example in X_test:\n",
        "    y = predict(input_example, feature_weights)\n",
        "    predictions.append(y)\n",
        "\n",
        "print (f\"EVALUATION of rule-based classifier is: {calculate_accuracy(y_test, predictions)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxv4-0mJIJS5",
        "outputId": "08bd267c-046b-4392-ca12-fc7a03607fdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVALUATION of rule-based classifier is: 50.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will be evaluated based on the performance of your rule-based classifier. Please note that the sample rule-based classifier achieves about 60% accuracy. Anything below 60% will not yield any points.\n",
        "\n",
        "***Please do not change the evaluation code in the block above, as it would be used to grade your classifier***."
      ],
      "metadata": {
        "id": "6u1EmkV5IaDR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below, we will directly learn the feature weights, rather than manually assigning those ourselves as manual assignment can be error-prone and labor intensive."
      ],
      "metadata": {
        "id": "7ZqIJHzbJiQe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FTuS2MOgDUU"
      },
      "outputs": [],
      "source": [
        "#@title Learning the weights of extracted features using logistic regression.\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "def get_sample_learnable_weights(X_data, Y_data, sample_extract_features):\n",
        "    \"\"\"\n",
        "    Learn feature weights using the training data.\n",
        "\n",
        "    Args:\n",
        "        X_data (list of strings): All the text data points in training data.\n",
        "        Y_data (list of int): Ground truth labels for text data points in X_data.\n",
        "        sample_extract_features: A Function that extracts features from text sample.\n",
        "                                 The sample_extract_features function should be of the\n",
        "                                 same format as sample_extract_features(X) function\n",
        "                                 implemented above.\n",
        "\n",
        "        Returns:\n",
        "            dictionary: feature names and their learned weights.\n",
        "    \"\"\"\n",
        "    # training a logistic regression model for classification using\n",
        "    # the features obtained by sample_extract_features(X) function.\n",
        "\n",
        "    # get all feature names\n",
        "    feature_names = list(sample_extract_features(X_data[0]).keys())\n",
        "\n",
        "    # Below code snippet accumulates features extracted from all the\n",
        "    # text data points in X_data.\n",
        "    all_features = []\n",
        "    # iterate over all text data points in X_data.\n",
        "    for input_example in X_data:\n",
        "      feature = [] # to store features extracted from input_example.\n",
        "      feat_dict = sample_extract_features(input_example)\n",
        "      # iterate over different feature names and store the corresonding values.\n",
        "      for name in feature_names:\n",
        "        feature.append(feat_dict[name])\n",
        "      all_features.append(feature) # append features obtained from input_example to all_features.\n",
        "\n",
        "    # Below, we show how to fit a logistic regression (LR) model using the features and the target labels.\n",
        "    # We use Sklearn's 'LogisticRegression' to do this. While initiating, 'fit_intercept' is set False,\n",
        "    # because 'bias' is already included in the extracted feature (see sample_extract_features() implementation).\n",
        "    # Also, random state is set to 42 to avoid different initialization of the model while running the codebook\n",
        "    # multiple times.\n",
        "    clf = LogisticRegression(fit_intercept=False, random_state=42).fit(all_features, Y_data)\n",
        "\n",
        "    # As training a logistic regression model assigns weights to each of the features (refer class notes),\n",
        "    # these weights are learnable and we will use them as learned feature weights.\n",
        "\n",
        "    # extract feature weights.\n",
        "    coffs = clf.coef_[0]\n",
        "\n",
        "    # convert to dictionary\n",
        "    coffs_dict = {feature_names[i]: coffs[i] for i in range(len(feature_names))}\n",
        "\n",
        "    return coffs_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsCdOCq6mtJ1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec838df1-198f-4e2c-efef-4b3c87b43c97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "63.775\n"
          ]
        }
      ],
      "source": [
        "#@title Putting the sample rule-based classifier with learnable weights together\n",
        "\n",
        "# get the sample weights\n",
        "sample_feature_weights_lr = get_sample_learnable_weights(X_train, y_train, sample_extract_features)\n",
        "\n",
        "predictions = []\n",
        "\n",
        "for input_example in X_test:\n",
        "    y = sample_predict(input_example, sample_feature_weights_lr)\n",
        "    predictions.append(y)\n",
        "\n",
        "print (calculate_accuracy(y_test, predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can observe here, the model achieves about 63% accuracy (note that the performance of a random classifier would be close to 50%). Note that, the performance significantly improves by making the weights learnable as compared to the manual weights."
      ],
      "metadata": {
        "id": "K29X_7C4zfVH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How does learnable weights-based classifier work on the features you implemented (3 marks)\n",
        "\n",
        "Your don't have to write anything here except simply running the below kernel.\n",
        "Depending upon the quality of your features, your classifier performance will vary."
      ],
      "metadata": {
        "id": "M7ztqIifzwud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Evaluating your rule-based classifier with learnable weights.\n",
        "\n",
        "## Please do not change anything in this code block.\n",
        "feature_weights_lr = get_sample_learnable_weights(X_train, y_train, extract_features)\n",
        "\n",
        "predictions = []\n",
        "\n",
        "for input_example in X_test:\n",
        "    y = predict(input_example, feature_weights_lr)\n",
        "    predictions.append(y)\n",
        "\n",
        "print (f\"EVALUATION of rule-based classifier with learnable weights is: {calculate_accuracy(y_test, predictions)}\")"
      ],
      "metadata": {
        "id": "FvVJYi980W0y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb262814-0151-409e-cf9c-a08ff6296b98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVALUATION of rule-based classifier with learnable weights is: 50.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2jMSsYLwY3L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "outputId": "a54e6df5-1aea-4246-c20a-f53cf2411194"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@title Visualization and comparison between manual and learnable weights.\n",
        "\n",
        "# visualize and compare the manual weights vs learnt weights.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "feature_names = list(sample_feature_weights.keys())\n",
        "manual_weights = [sample_feature_weights[name] for name in feature_names]\n",
        "learnt_weights = [sample_feature_weights_lr[name] for name in feature_names]\n",
        "weights_dict = {\n",
        "    'Manual weights': manual_weights,\n",
        "    'Learnt weights': learnt_weights,\n",
        "}\n",
        "\n",
        "x = np.arange(len(feature_names))  # the label locations\n",
        "width = 0.35  # the width of the bars\n",
        "multiplier = 0\n",
        "\n",
        "fig, ax = plt.subplots(layout='constrained')\n",
        "\n",
        "for attribute, measurement in weights_dict.items():\n",
        "    offset = width * multiplier\n",
        "    rects = ax.bar(x + offset, measurement, width, label=attribute)\n",
        "    ax.bar_label(rects, padding=3)\n",
        "    multiplier += 1\n",
        "\n",
        "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
        "ax.set_ylabel('weight')\n",
        "ax.set_title('Manual vs learnable weights comparison')\n",
        "ax.set_xticks(x + width, feature_names)\n",
        "ax.legend(loc='upper left', ncols=2)\n",
        "ax.set_ylim(-2, 2)\n",
        "\n",
        "plt.savefig('manual_vs_learned_weights.png')\n",
        "# Refer to the saved manual_vs_learned_weights.png to see how manual and learned weights compare."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TirWZWbRjnJH"
      },
      "source": [
        "### Approach 2: Bag-of-Words (BoW) (10 marks)\n",
        "\n",
        "The BoW vector representations is based on the unordered counts of words piece of text (similar to a \"bag\" of words).\n",
        "\n",
        "Let's attempt to build a classifier that tries to classify the reviews based on such vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TeavAClasyz6"
      },
      "outputs": [],
      "source": [
        "# implement your BoW classifier. In case the total number of words are very large,\n",
        "# consider using top-k most frequent words (e.g. k=10,000) while creating BoW vectors.\n",
        "\n",
        "# The below function finds out all words present in the corpus and assigns each word to\n",
        "# an index.\n",
        "\n",
        "def word_to_idx_BOW(X_data, K=10000):\n",
        "    \"\"\"\n",
        "    Generates a set of word-index pairs after analyzing all words present in entire\n",
        "    data corpus X_data.\n",
        "\n",
        "    Args:\n",
        "        X_data (list of strings): All the text data points in training data.\n",
        "        K (int): The number of most frequent words to be considered.\n",
        "                 In case (K-p)th to (K+q)th words in most frequent words order\n",
        "                 have same frequency, you can choose p words randomly from those\n",
        "                 (p+q) number of words.\n",
        "\n",
        "        Returns:\n",
        "            dictionary: words as keys and indices as values.\n",
        "    \"\"\"\n",
        "    word_to_idx = {}\n",
        "    # ADD YOUR CODE HERE\n",
        "    return word_to_idx\n",
        "\n",
        "# generate BoW feature for text input X.\n",
        "def extract_features_BoW(X, word_to_idx):\n",
        "    \"\"\"\n",
        "    Generates BoW feature for X using word_to_idx.\n",
        "\n",
        "    Args:\n",
        "        X (string): text input.\n",
        "        word_to_idx (dictionary): word-index mapping with words as keys and\n",
        "                                  indices as values.\n",
        "\n",
        "        Returns:\n",
        "            dictionary: features of X.\n",
        "    \"\"\"\n",
        "    features = {}\n",
        "    # ADD YOUR CODE HERE\n",
        "    return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HiruQeAvMr0",
        "outputId": "d319cdf1-0480-43bc-8ff6-0d3d56e49e2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{}\n"
          ]
        }
      ],
      "source": [
        "# checking the outputs of the above functions on a small set of examples\n",
        "\n",
        "sample_data = [\n",
        "    \"When is the homework due ?\",\n",
        "    \"When are the TAs' office hours ?\",\n",
        "    \"How hard is the homework ?\",\n",
        "]\n",
        "word_to_idx = word_to_idx_BOW(sample_data)\n",
        "features = extract_features_BoW(sample_data[0], word_to_idx)\n",
        "print(features)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Implement your feature weight learning for BoW classifier\n",
        "\n",
        "def get_learnable_weights_BoW(X_data, Y_data, word_to_idx_BOW, extract_features_BoW):\n",
        "    \"\"\"\n",
        "    Learning feature weights for BoW features, using training data.\n",
        "\n",
        "    Args:\n",
        "        X_data (list of strings): All the text data points in training data.\n",
        "        Y_data (list of int): Ground truth labels for text data points in X_data.\n",
        "        word_to_idx_BOW: A Function that looks at all the words in data corpus X_data\n",
        "                         and returns word-index mapping. You had to implement\n",
        "                         word_to_idx_BOW() funtion above.\n",
        "        extract_features_BoW: A Function that extracts BoW features from text sample.\n",
        "                              The extract_features_BoW() function had to be implemented\n",
        "                              by you above.\n",
        "\n",
        "        Returns:\n",
        "            dictionary: feature names and their learned weights.\n",
        "    \"\"\"\n",
        "\n",
        "    # ADD YOUR CODE HERE\n",
        "    coffs_dict = {}\n",
        "    return coffs_dict"
      ],
      "metadata": {
        "id": "ErmQIuFWTpa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Evaluating your BoW classifier\n",
        "\n",
        "def predict(X, feature_weights):\n",
        "    \"\"\"\n",
        "    Classifies the sentiment of a text input.\n",
        "\n",
        "    Args:\n",
        "        X (string): Text input.\n",
        "        feature_weights: weightage of different features.\n",
        "\n",
        "    Returns:\n",
        "        int: binary sentiment represented by 0/1.\n",
        "    \"\"\"\n",
        "    # ADD YOUR CODE HERE\n",
        "\n",
        "    return 1.0\n",
        "\n",
        "# get the sample weights\n",
        "BoW_feature_weights_lr = get_learnable_weights_BoW(X_train, y_train, word_to_idx_BOW, extract_features_BoW)\n",
        "\n",
        "predictions = []\n",
        "\n",
        "for input_example in X_test:\n",
        "    y = predict(input_example, BoW_feature_weights_lr)\n",
        "    predictions.append(y)\n",
        "\n",
        "print (f\"EVALUATION of BoW classifier is: {calculate_accuracy(y_test, predictions)}\")"
      ],
      "metadata": {
        "id": "eJSKciYNYNXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finding most positive and most negative words (2 Marks)\n",
        "\n",
        "Based on the magnitude of weights corresponding to different words, write down code to find the 5 most positive words and 5 most negative words."
      ],
      "metadata": {
        "id": "0DPrrDVmF4qs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "most_positive_words = []\n",
        "most_negative_words = []\n",
        "\n",
        "## WRITE CODE HERE TO POPULATE THESE LISTS\n",
        "## most_postive_words and most_negative_words should be list of strings\n",
        "\n",
        "assert (len(most_positive_words) == 5)\n",
        "assert (len(most_negative_words) == 5)\n",
        "\n",
        "print(\"EVALUATION five most positive words: \" + \" \".join(most_positive_words))\n",
        "print(\"EVALUATION five most negative words: \" + \" \".join(most_negative_words))"
      ],
      "metadata": {
        "id": "C6aX7UrbF_cx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77473227-6e29-4036-dfe2-bff1a88333da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EVALUATION five most positive words: good good good good good\n",
            "EVALUATION five most negative words: bad bad bad bad bad\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w48UudOSjLnh"
      },
      "source": [
        "## Part II Word2Vec (TA: Nicy Scaria)  (25 Marks)\n",
        "\n",
        "Word2vec is one of the most popular techniques to learn word embeddings. The idea behind word2vec was that the meaning of a word is determined by the context in which it occurs. A word embedding is a learned representation for text where words that have the same meaning have a similar representation.\n",
        "\n",
        "**Word2vec** model has 2 architectures:\n",
        "\n",
        "1. **Continuous bag of word (CBOW):**\n",
        "\n",
        "    CBOW predicts the center word from the surrounding context words.\n",
        "\n",
        "2. **Skip-gram:**\n",
        "\n",
        "    Skip-gram predicts surrounding context words from the center word."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baausB5gkhhT"
      },
      "source": [
        "#### SkipGram from Scratch\n",
        "\n",
        "In this exercise, you will code the skipgram model from scratch using the PyTorch library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LqI4ImZcP-I",
        "outputId": "1424fd65-89f0-4665-868e-b8f40d7bbc06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting portalocker\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: portalocker\n",
            "Successfully installed portalocker-2.8.2\n"
          ]
        }
      ],
      "source": [
        "!pip install portalocker #make sure you install the library and restart the session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pTSKA5fiVQs"
      },
      "outputs": [],
      "source": [
        "# importing the necessary libraries\n",
        "import torch\n",
        "from functools import partial\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext.data import to_map_style_dataset\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.datasets import WikiText2\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import json\n",
        "import argparse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Lu8GMxMlZ0f"
      },
      "source": [
        "> **In the following code the add your hyperparameters for the network.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZeXkPW6URMu"
      },
      "outputs": [],
      "source": [
        "# Initialization\n",
        "\n",
        "\n",
        "\"\"\"choose your hyperparameter and see the difference in performance\"\"\"\n",
        "\n",
        "# ADD YOUR CODE HERE\n",
        "\n",
        "# CHANGE THE None VALUES TO YOUR DESIRED VALUES\n",
        "# Please free to play with these hyperparameters to see the effects on the\n",
        "# quality of generated embeddings\n",
        "\n",
        "\n",
        "SKIPGRAM_N_WORDS = None # the length of the context on each side (k)\n",
        "\n",
        "MIN_WORD_FREQUENCY = None # only words with a minimum word frequency considered\n",
        "MAX_SEQUENCE_LENGTH = None # sentences with length more than this value truncated\n",
        "\n",
        "EMBED_DIMENSION = None # dimension of the word2vec vectors\n",
        "\n",
        "EMBED_MAX_NORM = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxyJ0Q1fT62V"
      },
      "outputs": [],
      "source": [
        "def get_english_tokenizer():\n",
        "    \"\"\"\n",
        "    Documentation:\n",
        "    https://pytorch.org/text/stable/_modules/torchtext/data/utils.html#get_tokenizer\n",
        "    \"\"\"\n",
        "    tokenizer = get_tokenizer(\"basic_english\", language=\"en\")\n",
        "    return tokenizer\n",
        "\n",
        "def get_data_iterator(ds_name, ds_type, data_dir):\n",
        "    \"\"\"\n",
        "    input dataset used:\n",
        "    https://paperswithcode.com/dataset/wikitext-2\n",
        "    This is directly imported from PyTorch.\n",
        "    \"\"\"\n",
        "    data_iter = WikiText2(root=data_dir, split=(ds_type))\n",
        "    data_iter = to_map_style_dataset(data_iter)\n",
        "    return data_iter\n",
        "\n",
        "def build_vocab(data_iter, tokenizer):\n",
        "    \"\"\"Builds vocabulary from iterator\"\"\"\n",
        "\n",
        "    vocab = build_vocab_from_iterator(\n",
        "        map(tokenizer, data_iter),\n",
        "        specials=[\"<unk>\"], #adding special tokens to the vocabulary\n",
        "        min_freq=MIN_WORD_FREQUENCY,\n",
        "    )\n",
        "    vocab.set_default_index(vocab[\"<unk>\"])\n",
        "    return vocab\n",
        "\n",
        "\n",
        "def collate_skipgram(batch, text_pipeline):\n",
        "    \"\"\"\n",
        "    This function prepares data for training the skipgram model.\n",
        "    It generates pairs of center and context words from the batch of text.\n",
        "\n",
        "    batch: A batch of text data\n",
        "    text_pipeline: A pipeline function that processes text into tokens\n",
        "    (batch_input, batch_output) -> (center word, context words)\n",
        "    \"\"\"\n",
        "    batch_input, batch_output = [], []\n",
        "\n",
        "    # Process each text in the batch\n",
        "    for text in batch:\n",
        "        text_tokens_ids = text_pipeline(text)\n",
        "\n",
        "        # Skip texts shorter than the required window size\n",
        "        if len(text_tokens_ids) < SKIPGRAM_N_WORDS * 2 + 1:\n",
        "            continue\n",
        "\n",
        "        # Truncate texts to a maximum sequence length if specified\n",
        "        if MAX_SEQUENCE_LENGTH:\n",
        "            text_tokens_ids = text_tokens_ids[:MAX_SEQUENCE_LENGTH]\n",
        "\n",
        "        # Create training pairs for each word in the text\n",
        "        for idx in range(len(text_tokens_ids) - SKIPGRAM_N_WORDS * 2):\n",
        "            token_id_sequence = text_tokens_ids[idx : (idx + SKIPGRAM_N_WORDS * 2 + 1)]\n",
        "            input_ = token_id_sequence.pop(SKIPGRAM_N_WORDS)\n",
        "            outputs = token_id_sequence\n",
        "\n",
        "            # Add each context word with the center word to the output lists\n",
        "            for output in outputs:\n",
        "                batch_input.append(input_)\n",
        "                batch_output.append(output)\n",
        "\n",
        "    # Convert lists to PyTorch tensors\n",
        "    batch_input = torch.tensor(batch_input, dtype=torch.long)\n",
        "    batch_output = torch.tensor(batch_output, dtype=torch.long)\n",
        "\n",
        "    return batch_input, batch_output\n",
        "\n",
        "\n",
        "def get_dataloader_and_vocab(\n",
        "    model_name, ds_name, ds_type, data_dir, batch_size, shuffle, vocab=None\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Prepares a DataLoader and builds a vocabulary for the dataset.\n",
        "    model_name: Name of the model to be used\n",
        "    ds_name: Name of the dataset\n",
        "    ds_type: Type of the dataset (e.g., train, test)\n",
        "    data_dir: Directory where the dataset is stored\n",
        "    batch_size: Size of each batch\n",
        "    vocab: An existing vocabulary, if available\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    data_iter = get_data_iterator(ds_name, ds_type, data_dir)\n",
        "    tokenizer = get_english_tokenizer()\n",
        "\n",
        "    if not vocab:\n",
        "        vocab = build_vocab(data_iter, tokenizer)\n",
        "\n",
        "    text_pipeline = lambda x: vocab(tokenizer(x))\n",
        "\n",
        "    collate_fn = collate_skipgram\n",
        "\n",
        "    # creates a DataLoader for the dataset\n",
        "\n",
        "    \"\"\"\n",
        "    dataloader documentation\n",
        "    https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n",
        "\n",
        "    \"\"\"\n",
        "    dataloader = DataLoader(\n",
        "        data_iter,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        collate_fn=partial(collate_fn, text_pipeline=text_pipeline),\n",
        "        )\n",
        "\n",
        "    return dataloader, vocab\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8Ap0SHCloMj"
      },
      "source": [
        "### Initialize the SkipGram Model\n",
        "\n",
        " **Complete the `initialization` and `forward` function in the following  SkipGram_Model class**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NNvSmAgYydH"
      },
      "outputs": [],
      "source": [
        "class SkipGram_Model(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementation of Skip-Gram model described in paper:\n",
        "    https://arxiv.org/abs/1301.3781\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size: int):\n",
        "        super(SkipGram_Model, self).__init__()\n",
        "\n",
        "        \"\"\"define the embedding and the linear layer of the network\"\"\"\n",
        "        # this is the initialization for the layers in the skipgram model\n",
        "\n",
        "        # ADD YOUR CODE HERE\n",
        "\n",
        "    def forward(self, inputs_):\n",
        "        \"\"\"define forward function\"\"\"\n",
        "        # ADD YOUR CODE HERE\n",
        "\n",
        "        # return the output of your final layer to find the minimize the loss\n",
        "\n",
        "        return\n",
        "\n",
        "    def get_word_embedding(self):\n",
        "        \"\"\" return the associated word embeddings for center words \"\"\"\n",
        "\n",
        "        # ADD YOUR CODE HERE\n",
        "\n",
        "        # return the normalized embeddings as a 2D numpy array (in word2vec models,\n",
        "        # the vector associated with the center word is considered\n",
        "        # word embedding)\n",
        "\n",
        "        return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcqxAiTSlwbw"
      },
      "source": [
        "> **The following is the Trainer class for the skip-gram model. Add your code for the `training` and `validation` loops.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wSYVSMlZFwA"
      },
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "    \"\"\"Main class for model training\"\"\"\n",
        "\n",
        "    # NOTE: you are free to add additional inputs/functions\n",
        "    # to the trainer class to make training better\n",
        "    # make sure to define and add it within the input\n",
        "    # and initialization if you are using any additional inputs\n",
        "    # for usage in the function\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model,\n",
        "        epochs,\n",
        "        train_dataloader,\n",
        "        val_dataloader,\n",
        "        criterion,\n",
        "        optimizer,\n",
        "        device,\n",
        "        model_dir,\n",
        "        model_name,\n",
        "    ):\n",
        "        self.model = model\n",
        "        self.epochs = epochs\n",
        "        self.train_dataloader = train_dataloader\n",
        "        self.val_dataloader = val_dataloader\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.device = device\n",
        "        self.model_dir = model_dir\n",
        "        self.model_name = model_name\n",
        "        self.loss = {\"train\": [], \"val\": []}\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    # ADD YOUR CODE HERE FOR TRAINING & VALIDATION\n",
        "    # This implementation need not include negative sampling.\n",
        "\n",
        "    # NOTE: you can add additional functions to make training better\n",
        "\n",
        "\n",
        "    def save_model(self):\n",
        "        \"\"\"\n",
        "        Save final model to directory\n",
        "\n",
        "        \"\"\"\n",
        "        model_path = os.path.join(self.model_dir, \"model.pt\")\n",
        "        torch.save(self.model, model_path)\n",
        "\n",
        "    def save_loss(self):\n",
        "        \"\"\"\n",
        "        Save train/val loss as json file to the directory\n",
        "\n",
        "        \"\"\"\n",
        "        loss_path = os.path.join(self.model_dir, \"loss.json\")\n",
        "        with open(loss_path, \"w\") as fp:\n",
        "            json.dump(self.loss, fp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BgM-DGTl4qy"
      },
      "source": [
        "> **The following code block defines the various parameters and nomenclature for the training and saving of the skip-gram model. Add numerical values for the `specified hyperparameters`.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzoqreghaSgc"
      },
      "outputs": [],
      "source": [
        "# ADD YOUR CODE HERE\n",
        "# CHANGE THE None VALUES TO YOUR DESIRED VALUES\n",
        "\n",
        "model_name = 'skipgram'\n",
        "\n",
        "dataset = 'WikiText2'\n",
        "data_dir = './data/'\n",
        "train_batch_size = None\n",
        "val_batch_size = None\n",
        "shuffle = True\n",
        "\n",
        "optimizer = None\n",
        "learning_rate = None\n",
        "epochs = None\n",
        "\n",
        "# ADD YOUR CODE HERE\n",
        "# change the directory name with your SAPname and SRno\n",
        "\n",
        "model_dir = 'SAPname_SRno'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSE1qsvxmEdP"
      },
      "source": [
        "> **The following code block is used to train and save the model. Add the code wherever required**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zMDRVHMZWO6"
      },
      "outputs": [],
      "source": [
        "os.makedirs(model_dir)\n",
        "\n",
        "train_dataloader, vocab = get_dataloader_and_vocab(\n",
        "    model_name=model_name,\n",
        "    ds_name=dataset,\n",
        "    ds_type=\"train\",\n",
        "    data_dir=data_dir,\n",
        "    batch_size=train_batch_size,\n",
        "    shuffle=shuffle,\n",
        "    vocab=None,\n",
        ")\n",
        "\n",
        "val_dataloader, _ = get_dataloader_and_vocab(\n",
        "    model_name=model_name,\n",
        "    ds_name=dataset,\n",
        "    ds_type=\"valid\",\n",
        "    data_dir=data_dir,\n",
        "    batch_size=val_batch_size,\n",
        "    shuffle=shuffle,\n",
        "    vocab=vocab,\n",
        ")\n",
        "\n",
        "vocab_size = len(vocab.get_stoi())\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "\n",
        "model_class = SkipGram_Model\n",
        "model = model_class(vocab_size=vocab_size)\n",
        "\n",
        "# ADD YOUR CODE HERE\n",
        "# You'll have to specify the loss criterion\n",
        "# Your loss function would depend upon whether you\n",
        "# choose to train with negative sampling or not\n",
        "# either of the two are valid choices\n",
        "\n",
        "criterion = None\n",
        "\n",
        "# ADD YOUR CODE HERE\n",
        "# You'll have to specify the optimizer here\n",
        "optimizer = None\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# NOTE: if you are **optionally** using additional options for the trainer\n",
        "# (e.g., a training scheduler), please add them below.\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    epochs=epochs,\n",
        "    train_dataloader=train_dataloader,\n",
        "    val_dataloader=val_dataloader,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    device=device,\n",
        "    model_dir=model_dir,\n",
        "    model_name=model_name,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "print(\"Training finished.\")\n",
        "\n",
        "trainer.save_model()\n",
        "trainer.save_loss()\n",
        "vocab_path = os.path.join(model_dir, \"vocab.pt\")\n",
        "torch.save(vocab, vocab_path)\n",
        "print(\"Model artifacts saved to folder:\", model_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1PKIac1mMsH"
      },
      "source": [
        "### Let us analyze the performance of the model\n",
        "\n",
        "You'll be evaluated on the quality of the word representations as judged by the word similarity test, and word analogy tests.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-K93nQt5CTZ"
      },
      "outputs": [],
      "source": [
        "#@title Evaluation\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import sys\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80d1lRiD7ms4"
      },
      "outputs": [],
      "source": [
        "# ADD YOUR CODE HERE\n",
        "# change the directory name with your SAPname and SRno\n",
        "\n",
        "folder = \"SAPname_SRno\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# load the saved model\n",
        "model = torch.load(f\"{folder}/model.pt\", map_location=device)\n",
        "vocab = torch.load(f\"{folder}/vocab.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_embeddings = model.get_word_embedding()\n",
        "\n",
        "# ADD YOUR CODE HERE\n",
        "# change the directory name with your SAPname and SRno\n",
        "\n",
        "# Save the embeddings to the folder\n",
        "np.save('SAPname_SRno/word_embeddings.npy', word_embeddings)"
      ],
      "metadata": {
        "id": "Wc9AMn5GVUKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the embeddings are trained, we can use a few words to evaluate some desirable properties of word representations.\n",
        "\n",
        "For instance, whether similar words are indeed similar in the high-dimensional space?"
      ],
      "metadata": {
        "id": "21pPrbet--qJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7hq6o3z777b"
      },
      "outputs": [],
      "source": [
        "words = ['king', 'queen', 'river', 'water', 'ocean', 'tree', 'plant', 'happy', 'glad', 'mother', 'daughter']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5kAEr9rmYym"
      },
      "source": [
        "> **Write a code to find the similarity of the each word in words with eachother**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCXVzRrh8Dvu"
      },
      "outputs": [],
      "source": [
        "def get_word_similarity(words):\n",
        "  \"\"\"\n",
        "  This function takes the words as input and outputs the word vectors\n",
        "  corresponding to the words obtained from your word2vec model and the\n",
        "  similarity of every word with each other.\n",
        "  word2vec is the embedding matrix corresponding to the given words and\n",
        "  this has to be returned as a numpy array to apply PCA on it whereas,\n",
        "  w2v_similarity[i][j] should contain the similarity of word i with word j\n",
        "\n",
        "  \"\"\"\n",
        "  # ADD YOUR CODE HERE\n",
        "\n",
        "  # you'll have to compute the similarity matrix for the words given above\n",
        "\n",
        "  return word2vec, w2v_similarity\n",
        "\n",
        "word2vec, w2v_similarity = get_word_similarity(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrUjGgstmhx8"
      },
      "source": [
        "Let us visualize this similarity matrix. The similarity of each word with other words in words is displayed as a pandas dataframe and as a heatmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8OVhSUZ8LZT"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(w2v_similarity, columns = words, index = words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9AWL0kqAmqsK"
      },
      "outputs": [],
      "source": [
        "sns.heatmap(pd.DataFrame(w2v_similarity, columns = words, index = words))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V12MD_EumuA7"
      },
      "source": [
        "The size of the words embedding are reduced to to 2D and displayed as a scatterplot for analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AF9qSiwNmwzw"
      },
      "outputs": [],
      "source": [
        "# Create a 2-dimensional PCA model of the word vectors using the scikit-learn PCA class\n",
        "# n_components in PCA specifies the no.of dimensions\n",
        "pca = PCA(n_components=2)\n",
        "\n",
        "# Fit and transform the vectors using PCA model\n",
        "reduced_w2v = pca.fit_transform(word2vec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kS9QF8AE8MTG"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,5))\n",
        "plt.scatter(reduced_w2v[:,0],reduced_w2v[:,1], s = 12, color = 'red')\n",
        "plt.xlim([-2,2])\n",
        "plt.ylim([-2,2])\n",
        "x, y = reduced_w2v[:,0] , reduced_w2v[:,1]\n",
        "offset = 0.5\n",
        "for i in range(len(x)):\n",
        "    label = words[i]\n",
        "    xi, yi = x[i], y[i]\n",
        "    plt.annotate(label, (xi, yi), xytext=(xi + offset, yi + offset),\n",
        "                 textcoords='offset points', ha='center', va='center')\n",
        "\n",
        "plt.savefig(\"word_similarity.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4w3UUE1m4W_"
      },
      "source": [
        "The 10 most similar word to the given word is calculated in the following code blocks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvXS4uzd8O7W"
      },
      "outputs": [],
      "source": [
        "def get_top_similar(word: str, topN: int = 10):\n",
        "\n",
        "    \"\"\"\n",
        "    This function calculates the topN words similar to the input word.\n",
        "    If the word is not in vocabulary, then similarity is not calculated.\n",
        "    If the word is in the vocabulary, then the dot product of the embedding\n",
        "    matrix and the word vector is calculated. The topN words are selected.\n",
        "    \"\"\"\n",
        "    word_id = vocab[word]\n",
        "    if word_id == 0:\n",
        "        print(\"Out of vocabulary word\")\n",
        "        return\n",
        "\n",
        "    word_vec = model.get_word_embedding[word_id]\n",
        "    word_vec = np.reshape(word_vec, (len(word_vec), 1))\n",
        "\n",
        "    dists = np.matmul(model.get_word_embedding, word_vec).flatten()\n",
        "    topN_ids = np.argsort(-dists)[1 : topN + 1]\n",
        "\n",
        "    topN_dict = {}\n",
        "    for sim_word_id in topN_ids:\n",
        "        sim_word = vocab.lookup_token(sim_word_id)\n",
        "        topN_dict[sim_word] = dists[sim_word_id]\n",
        "\n",
        "    return topN_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atY4cy0J8Tfi"
      },
      "outputs": [],
      "source": [
        "for word, sim in get_top_similar(\"india\").items():\n",
        "   print(\"EVALUATION: most similar words to {}: {:.3f}\".format(word, sim))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iz-elU-5nAL1"
      },
      "source": [
        "### Analogy Tests\n",
        "\n",
        "Analogy tests include questions of the format a:b::x:?, such tests are used to intrinsically evaluate the quality of word vectors.\n",
        "\n",
        "Here's one example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KrTVR2BnC9N"
      },
      "outputs": [],
      "source": [
        "def get_analogy(word_1, word_2, word_3):\n",
        "\n",
        "  \"\"\"\n",
        "  top 5 most analgous vector calculated correspond to a set similar to\n",
        "  man: woman :: king: ? . This is calculated similar to the above case.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  emb1 = model.get_word_embedding[vocab[word_1]]\n",
        "  emb2 = model.get_word_embedding[vocab[word_2]]\n",
        "  emb3 = model.get_word_embedding[vocab[word_3]]\n",
        "\n",
        "  emb4 = emb1 - emb2 + emb3\n",
        "\n",
        "  # compute dot products between 'emb4' and all word embeddings in the model.\n",
        "  emb4 = np.reshape(emb4, (len(emb4), 1))\n",
        "  dot_product = np.matmul(model.get_word_embedding, emb4).flatten()\n",
        "\n",
        "  top5 = np.argsort(-dot_product)[:5]\n",
        "\n",
        "  return top5, dot_product\n",
        "\n",
        "top5_analogy, dot_product = get_analogy('king', 'man', 'woman')\n",
        "\n",
        "for word_id in top5_analogy:\n",
        "    print(\"{}: {:.3f}\".format(vocab.lookup_token(word_id), dot_product[word_id]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKTVUVPEnGVN"
      },
      "source": [
        "The model performance will be evaluated based on an analogy output for the top 5 words. The following code will used to evaluate the performance of the model on the analogies dataset. We will measure how often the correct answer is a part of the top 5 options."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading the file containing a few analogies.\n",
        "# We will change the contents of this file while testing.\n",
        "\n",
        "!wget -O analogies.txt \"https://drive.google.com/uc?export=download&id=1jHx25dECegjtRKBB587nEfHiJesrH0g2\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2hbpb_h5KgN",
        "outputId": "51993f16-153e-48e9-b218-6c399b32db41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-22 11:22:20--  https://drive.google.com/uc?export=download&id=1jHx25dECegjtRKBB587nEfHiJesrH0g2\n",
            "Resolving drive.google.com (drive.google.com)... 74.125.31.101, 74.125.31.102, 74.125.31.100, ...\n",
            "Connecting to drive.google.com (drive.google.com)|74.125.31.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1jHx25dECegjtRKBB587nEfHiJesrH0g2&export=download [following]\n",
            "--2024-01-22 11:22:20--  https://drive.usercontent.google.com/download?id=1jHx25dECegjtRKBB587nEfHiJesrH0g2&export=download\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 74.125.141.132, 2607:f8b0:400c:c06::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|74.125.141.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 286075 (279K) [application/octet-stream]\n",
            "Saving to: ‘analogies.txt’\n",
            "\n",
            "analogies.txt       100%[===================>] 279.37K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2024-01-22 11:22:20 (89.7 MB/s) - ‘analogies.txt’ saved [286075/286075]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.seed(42)\n",
        "\n",
        "def load_and_sample_analogies(file_path, sample_size=5000):\n",
        "    with open(file_path, 'r') as file:\n",
        "        analogies = []\n",
        "        for line in file:\n",
        "            # Split the line into words and ensure it has exactly 4 elements\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) == 4:\n",
        "                analogies.append(parts)\n",
        "\n",
        "        # Sample analogies\n",
        "        sampled_analogies = random.sample(analogies, min(sample_size, len(analogies)))\n",
        "        return sampled_analogies\n",
        "\n",
        "# Path to your text file\n",
        "# NOTE: analogies used for grading could be slightly different\n",
        "file_path = '/content/analogies.txt'\n",
        "\n",
        "# Load and sample analogies\n",
        "sampled_analogy_dataset = load_and_sample_analogies(file_path)"
      ],
      "metadata": {
        "id": "obbMdoMe1Tw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_word_id(word, vocab):\n",
        "    \"\"\"check for out of vocabulary items\"\"\"\n",
        "    return vocab[word] if word in vocab else 0\n",
        "\n",
        "def analogy_score(analogy_dataset):\n",
        "    \"\"\"\n",
        "    The top5 analogous words calculated for each set of words for your\n",
        "    implementation of word2vec and compared with an existing dataset to\n",
        "    calculate if the expected word is in the first 5 predictions.\n",
        "    \"\"\"\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for a, b, c, d in analogy_dataset:\n",
        "        # Convert words to lowercase\n",
        "        a, b, c, d = a.lower(), b.lower(), c.lower(), d.lower()\n",
        "\n",
        "        # Check if any word is out of vocabulary\n",
        "        if 0 in [get_word_id(word, vocab) for word in [a, b, c, d]]:\n",
        "            continue\n",
        "\n",
        "        #finding the first five words that are analogous to the given set\n",
        "        top5_analogy, dot_product = get_analogy(a, b, c)\n",
        "\n",
        "        predicted_words = []\n",
        "\n",
        "        for word_id in top5_analogy:\n",
        "            word = vocab.lookup_token(word_id)\n",
        "            predicted_words.append(word)\n",
        "\n",
        "        if d in predicted_words:\n",
        "            correct += 1\n",
        "\n",
        "        total += 1\n",
        "\n",
        "    precision_at_5 = correct / total if total > 0 else 0\n",
        "    return precision_at_5\n",
        "\n",
        "precision_at_5 = analogy_score(sampled_analogy_dataset)\n",
        "print(\"EVALUATION: Precision at 5 for the analogy test is\", precision_at_5)"
      ],
      "metadata": {
        "id": "TuLvO2cV1W2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwjiemeOnKvj"
      },
      "source": [
        "#### Google's word2vec for comparison\n",
        "\n",
        "In the following code blocks, the pretained word2vec developed by Google is used to analyze the quality of the embedding. The word2vec model can be downloaded from [here](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O GoogleNews-vectors-negative300.bin \"https://drive.google.com/uc?export=download&id=12Oicgl5scdJLR7t8jbzKpW6o8QkYOylg\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KhiakvJ3wwe",
        "outputId": "b833be2d-b1b8-4417-c575-454b40191526"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-22 11:52:16--  https://drive.google.com/uc?export=download&id=12Oicgl5scdJLR7t8jbzKpW6o8QkYOylg\n",
            "Resolving drive.google.com (drive.google.com)... 74.125.31.100, 74.125.31.102, 74.125.31.101, ...\n",
            "Connecting to drive.google.com (drive.google.com)|74.125.31.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=12Oicgl5scdJLR7t8jbzKpW6o8QkYOylg&export=download [following]\n",
            "--2024-01-22 11:52:16--  https://drive.usercontent.google.com/download?id=12Oicgl5scdJLR7t8jbzKpW6o8QkYOylg&export=download\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 74.125.141.132, 2607:f8b0:400c:c06::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|74.125.141.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2446 (2.4K) [text/html]\n",
            "Saving to: ‘GoogleNews-vectors-negative300.bin’\n",
            "\n",
            "\r          GoogleNew   0%[                    ]       0  --.-KB/s               \rGoogleNews-vectors- 100%[===================>]   2.39K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-01-22 11:52:16 (17.9 MB/s) - ‘GoogleNews-vectors-negative300.bin’ saved [2446/2446]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4\n",
        "import requests\n",
        "import tqdm.auto as tqdm\n",
        "\n",
        "def download_from_drive(drive_link, target_path):\n",
        "    response = requests.get(drive_link, stream=True)\n",
        "    response.raise_for_status()\n",
        "    if 'html' in response.headers['Content-Type']:\n",
        "        response = requests.get(drive_link)\n",
        "        response.raise_for_status()\n",
        "        page = bs4.BeautifulSoup(response.text, features=\"lxml\")\n",
        "        if form := page.find('form', id='download-form'):\n",
        "            id   = form.select_one(\"input[name='id']\")['value']\n",
        "            uuid = form.select_one(\"input[name='uuid']\")['value']\n",
        "            data = { 'confirm': 't', 'export': 'download', 'id': id, 'uuid': uuid }\n",
        "            response = requests.get(page.find('form')['action'], params=data, stream=True)\n",
        "            response.raise_for_status()\n",
        "    with open(target_path, 'wb+') as file:\n",
        "        with tqdm.tqdm(\n",
        "            total=int(response.headers['Content-Length']),\n",
        "            unit='B', unit_scale=True, unit_divisor=1024\n",
        "        ) as pbar:\n",
        "            for chunk in response.iter_content(chunk_size=4096):\n",
        "                file.write(chunk)\n",
        "                pbar.update(len(chunk))\n",
        "    print(\"Downloaded to\", target_path)\n",
        "\n",
        "drive_link = \"https://drive.google.com/uc?export=download&id=12Oicgl5scdJLR7t8jbzKpW6o8QkYOylg\"\n",
        "target_path = \"/content/GoogleNews-vectors-negative300.bin\"\n",
        "\n",
        "download_from_drive(drive_link, target_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "c8bca5cf80de4948ad86ed9b08469b75",
            "3d0983ae13a34bd9a1635309867cb9cd",
            "c8fbb3415ad941d0b138bc5743f77ac3",
            "b2eadb5f487e45f983771b9c4601ee78",
            "89b7ea42b66e4ea5952f13cea491b9bf",
            "c3053e733101432985d8a9e480d2a29b",
            "de9d707864cd47c28ce47383890c023b",
            "499b50c69b2a43e18aacf4214ecd5b7c",
            "27d2150a6c6a4291bcfee282f491df88",
            "b81d3faa449543a183922d2eafefe47e",
            "93633618c22a48b88dda45059886daf4"
          ]
        },
        "id": "KNMjvv4K2xd0",
        "outputId": "3b03f980-b74e-4a11-cd70-7d62f60007e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/3.39G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c8bca5cf80de4948ad86ed9b08469b75"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded to /content/GoogleNews-vectors-negative300.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzoahZOUnQIO"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "\n",
        "# Load Google news 300 vectors file\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True, limit=500000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ep71XY238WpV"
      },
      "outputs": [],
      "source": [
        "# List of words to plot the embeddings\n",
        "words = ['king', 'queen', 'river', 'water', 'ocean', 'tree', 'plant', 'happy', 'glad', 'mother', 'daughter']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLUtqxPrnWV9"
      },
      "source": [
        "> **Write a code to find the similarity of the each word in words with eachother using original word2vec**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4QLcfJvvnYl9"
      },
      "outputs": [],
      "source": [
        "def get_word_similarity(words):\n",
        "  \"\"\"\n",
        "  This function takes the words as input and outputs the word vectors\n",
        "  corresponding to the words using Google's word2vec and the similarity of\n",
        "  every word with eachother. word2vec is the embedding matrix for the words\n",
        "  given above w2v_similarity[i][j] should contain the similarity of word i with j\n",
        "\n",
        "  \"\"\"\n",
        "  # ADD YOUR CODE HERE\n",
        "\n",
        "  # you'll have to compute the similarity matrix for the words given above\n",
        "\n",
        "  return word2vec, w2v_similarity\n",
        "\n",
        "word2vec, w2v_similarity = get_word_similarity(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLQn4B_snbfg"
      },
      "source": [
        "The similarity of each word with other words in words is displayed as a pandas dataframe and as a heatmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3-NqzVunhg1"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(w2v_similarity, columns = words, index = words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "koErrEInnjcd"
      },
      "outputs": [],
      "source": [
        "sns.heatmap(pd.DataFrame(w2v_similarity, columns = words, index = words))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptKTusGJnpLy"
      },
      "source": [
        "The size of the words embedding are reduced to to 2D and displayed as a scatterplot for analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADEMWok8nrX0"
      },
      "outputs": [],
      "source": [
        "#PCA on word2vec embedding\n",
        "pca = PCA(n_components=2)\n",
        "\n",
        "reduced_w2v = pca.fit_transform(word2vec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "la2osFTxntIc"
      },
      "outputs": [],
      "source": [
        "#plotting reduced order embeddings in a 2-D space\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.scatter(reduced_w2v[:,0],reduced_w2v[:,1], s = 12, color = 'red')\n",
        "plt.xlim([-2.5,2.5])\n",
        "plt.ylim([-2.5,2.5])\n",
        "x, y = reduced_w2v[:,0] , reduced_w2v[:,1]\n",
        "for i in range(len(x)):\n",
        "    plt.annotate(words[i],xy=(x[i], y[i]),xytext=(x[i]+0.05,y[i]+0.05))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Whg_kv2QnvXW"
      },
      "outputs": [],
      "source": [
        "model.most_similar('india')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIx8JEMynzK5"
      },
      "source": [
        "Analogy test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N41c0AJKnzou"
      },
      "outputs": [],
      "source": [
        "def analogy(x1, x2, y1): #defining analogy function\n",
        "    result = model.most_similar(positive=[y1, x2], negative=[x1], topn = 5)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbEfS_7An3RF"
      },
      "outputs": [],
      "source": [
        "analogy('man', 'king', 'woman')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def analogy_score(analogy_dataset):\n",
        "\n",
        "    \"\"\"\n",
        "    The top5 analogous words calculated for each set of words for Google's\n",
        "    word2vec and compared with an existing dataset to calculate if the word\n",
        "    is in the first 5 predictions.\n",
        "\n",
        "    \"\"\"\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for a, b, c, d in analogy_dataset:\n",
        "        # Convert words to lowercase\n",
        "        a, b, c, d = a.lower(), b.lower(), c.lower(), d.lower()\n",
        "\n",
        "        words_scores = analogy(a,b,c)\n",
        "\n",
        "        predicted_words = [item[0] for item in words_scores]\n",
        "\n",
        "        if d in predicted_words:\n",
        "            correct += 1\n",
        "\n",
        "        total += 1\n",
        "\n",
        "    precision_at_5 = correct / total if total > 0 else 0\n",
        "    return precision_at_5\n",
        "\n",
        "precision_at_5_Google = analogy_score(sampled_analogy_dataset)\n",
        "print(\"EVALUATION: Precision at 5 for the analogy test with Google skip-gram model is\", precision_at_5_Google)"
      ],
      "metadata": {
        "id": "wbqo3EhNy952"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acp_DOcZn5j2"
      },
      "source": [
        "### Submission Instructions\n",
        "\n",
        "Mentioned at the top of the notebook."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c8bca5cf80de4948ad86ed9b08469b75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3d0983ae13a34bd9a1635309867cb9cd",
              "IPY_MODEL_c8fbb3415ad941d0b138bc5743f77ac3",
              "IPY_MODEL_b2eadb5f487e45f983771b9c4601ee78"
            ],
            "layout": "IPY_MODEL_89b7ea42b66e4ea5952f13cea491b9bf"
          }
        },
        "3d0983ae13a34bd9a1635309867cb9cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3053e733101432985d8a9e480d2a29b",
            "placeholder": "​",
            "style": "IPY_MODEL_de9d707864cd47c28ce47383890c023b",
            "value": "100%"
          }
        },
        "c8fbb3415ad941d0b138bc5743f77ac3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_499b50c69b2a43e18aacf4214ecd5b7c",
            "max": 3644258522,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_27d2150a6c6a4291bcfee282f491df88",
            "value": 3644258522
          }
        },
        "b2eadb5f487e45f983771b9c4601ee78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b81d3faa449543a183922d2eafefe47e",
            "placeholder": "​",
            "style": "IPY_MODEL_93633618c22a48b88dda45059886daf4",
            "value": " 3.39G/3.39G [01:08&lt;00:00, 56.6MB/s]"
          }
        },
        "89b7ea42b66e4ea5952f13cea491b9bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3053e733101432985d8a9e480d2a29b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de9d707864cd47c28ce47383890c023b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "499b50c69b2a43e18aacf4214ecd5b7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27d2150a6c6a4291bcfee282f491df88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b81d3faa449543a183922d2eafefe47e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93633618c22a48b88dda45059886daf4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}